[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "charlotte.computer",
    "section": "",
    "text": "I am a human named Charlotte who is studying to be an AI Engineer.\nHere is where topics are organised and resources are shared for other humans that may also like to learn about AI\nTopics are added from Job descriptions I’d like to be qualified for, topics are added to overtime.\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\nPerceptrons\n\n\n\n\n\n\n\n\n\n\n\n\nTokenization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "topics/perceptrons.html",
    "href": "topics/perceptrons.html",
    "title": "Perceptrons",
    "section": "",
    "text": "perceptrons do have logic gates for AND, OR, and NOT.\nHowever…there is no solution for XOR gate.\n\nUntil we thought about Networks of Perceptrons. Networked elements are required.\nThere is a need for three perceptrons to solve the XOR gate.\n\nWhen the inidividual outputs of a layer of perceptrons is not needed to be visuallised, we call it a Hidden Layer.\nOnce you begin networking the perceptions, you can perform any boolean function.\nThis is deemed a multi-layer perceptron. Perceptrons are arranged in layers.\n\nLinear Classifier\nA perceptron operates on real-valued vectors.\nThere is a boundary where all inputs are classified as 0 or 1.\n\nA perceptron defines a boundary (the line and the area) where on the graph its a 0 or 1 on each side of that linear classifier line.\nYou can create a shape with many perceptrons with their own linear classifiers. You create a boundary.\nSo you create a region where all perceptrons must output a 1.\n\nDecision Boundaries\n\n\n\nFinding and fitting a decision boundary to the data is one of the main objectives of Machine Learning\n\n\n\nConverges, Coeficiants\nFor each misclassifcation, we adjust the coeficiants to move the boundary in the direction of the misclassification. If still not classified correctly, we adjust the coeficiants again. This process is called Gradient Descent.\n\n\nTensorFlow Playground\n\nBuild a network that isolates the region within the region we wish to classify.\n\n\nIndividual perceptrons capture linear boundaries\n\n\n\n\nx_input = [0.1, 0.5, 0.2]\nw_weights = [0.4, 0.3, 0.6]\nthreshold = 0.5\n\ndef step_function(weighted_sum):\n    if weighted_sum &gt; threshold:\n        return 1\n    else:\n        return 0\n\ndef perceptron():\n    weighted_sum = 0 \n    for x,w in zip(x_input, w_weights):\n        weighted_sum += x*w\n        print(weighted_sum)\n    return step_function(weighted_sum)\n\noutput = perceptron()\nprint(\"Output:\", str(output))\n\n\n0.04000000000000001\n0.19\n0.31\nOutput: 0\n\n\nEach iteration is increasing our weighted sum, at the end we reached 0.31 which was smaller than the threshold of 0.5, therefore the output is 0\n\n\nThe perceptron is behaving like a Linear Classifier",
    "crumbs": [
      "Perceptrons"
    ]
  },
  {
    "objectID": "topics/tokenization.html",
    "href": "topics/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "Byte-Pair Encoding Algorithm\nHere is a live Tokenizer called Tiktokenizer\ndef get_most_frequent_pair(ids):\n    # count the frequency of each pair of bytes\n    pair_counts = {}\n    for pair in zip(ids, ids[1:]): # zip() pairs each byte with the next one\n        pair_counts[pair] = pair_counts.get(pair, 0) + 1 # get() returns the value of the pair, or 0 if it doesn't exist\n    return pair_counts\n\nmost_frequent_pair = get_most_frequent_pair(tokens)\n# print(most_frequent_pair)\nprint(sorted(((value, key) for key, value in most_frequent_pair.items()), reverse=True))\n\n[(57, (101, 32)), (52, (115, 32)), (48, (105, 110)), (40, (32, 116)), (40, (32, 97)), (35, (110, 103)), (31, (111, 110)), (29, (32, 111)), (28, (116, 105)), (26, (114, 101)), (26, (97, 116)), (25, (116, 104)), (24, (101, 114)), (24, (97, 110)), (23, (100, 32)), (23, (44, 32)), (22, (104, 101)), (21, (101, 115)), (21, (32, 109)), (20, (105, 111)), (20, (103, 32)), (19, (111, 100)), (19, (110, 32)), (19, (99, 97)), (19, (32, 99)), (18, (111, 102)), (18, (108, 105)), (18, (100, 101)), (18, (32, 65)), (17, (116, 32)), (17, (110, 101)), (17, (108, 101)), (17, (105, 108)), (17, (104, 97)), (17, (102, 32)), (16, (110, 100)), (16, (101, 110)), (16, (97, 115)), (16, (65, 73)), (15, (111, 32)), (15, (110, 115)), (15, (109, 111)), (15, (105, 99)), (15, (73, 32)), (15, (46, 32)), (15, (32, 112)), (15, (32, 98)), (14, (121, 32)), (14, (116, 111)), (14, (115, 101)), (14, (111, 114)), (14, (101, 108)), (14, (100, 105)), (14, (32, 102)), (13, (112, 108)), (13, (97, 112)), (13, (32, 105)), (13, (32, 101)), (11, (119, 32)), (11, (114, 111)), (11, (105, 116)), (11, (101, 97)), (11, (97, 108)), (11, (32, 119)), (10, (112, 114)), (10, (112, 112)), (10, (108, 115)), (10, (105, 115)), (10, (103, 101)), (10, (101, 100)), (10, (32, 115)), (10, (32, 104)), (10, (32, 100)), (9, (118, 101)), (9, (114, 32)), (9, (110, 99)), (9, (109, 97)), (9, (104, 105)), (9, (101, 119)), (9, (101, 99)), (9, (99, 111)), (9, (99, 104)), (9, (32, 114)), (8, (116, 101)), (8, (115, 116)), (8, (115, 44)), (8, (114, 97)), (8, (102, 111)), (8, (97, 98)), (8, (32, 108)), (7, (118, 105)), (7, (117, 116)), (7, (117, 115)), (7, (114, 105)), (7, (111, 119)), (7, (111, 117)), (7, (111, 109)), (7, (105, 101)), (7, (103, 105)), (7, (100, 97)), (7, (99, 101)), (7, (98, 101)), (7, (97, 114)), (7, (97, 105)), (7, (32, 117)), (7, (32, 110)), (6, (226, 128)), (6, (119, 104)), (6, (117, 105)), (6, (110, 116)), (6, (110, 105)), (6, (109, 101)), (6, (108, 121)), (6, (108, 97)), (6, (108, 32)), (6, (99, 116)), (6, (97, 32)), (5, (119, 105)), (5, (117, 114)), (5, (116, 97)), (5, (115, 115)), (5, (114, 103)), (5, (112, 111)), (5, (112, 32)), (5, (111, 118)), (5, (111, 112)), (5, (108, 117)), (5, (108, 108)), (5, (108, 100)), (5, (104, 111)), (5, (102, 101)), (5, (101, 101)), (5, (99, 114)), (5, (98, 108)), (5, (97, 100)), (4, (128, 153)), (4, (118, 97)), (4, (117, 110)), (4, (117, 101)), (4, (117, 99)), (4, (116, 117)), (4, (116, 114)), (4, (115, 105)), (4, (115, 99)), (4, (114, 118)), (4, (114, 110)), (4, (112, 97)), (4, (111, 115)), (4, (105, 122)), (4, (104, 32)), (4, (102, 117)), (4, (101, 118)), (4, (101, 109)), (4, (101, 44)), (4, (99, 108)), (4, (98, 117)), (4, (97, 118)), (4, (97, 109)), (4, (84, 104)), (4, (32, 84)), (3, (119, 101)), (3, (117, 100)), (3, (116, 44)), (3, (115, 46)), (3, (114, 115)), (3, (113, 117)), (3, (112, 101)), (3, (111, 111)), (3, (110, 121)), (3, (110, 111)), (3, (109, 105)), (3, (108, 111)), (3, (102, 114)), (3, (102, 102)), (3, (101, 120)), (3, (101, 46)), (3, (100, 117)), (3, (99, 105)), (3, (98, 111)), (3, (98, 105)), (3, (97, 121)), (3, (97, 103)), (3, (97, 99)), (3, (73, 226)), (3, (32, 103)), (3, (32, 73)), (2, (153, 108)), (2, (128, 148)), (2, (122, 97)), (2, (121, 44)), (2, (120, 112)), (2, (117, 112)), (2, (117, 108)), (2, (117, 97)), (2, (116, 121)), (2, (115, 111)), (2, (115, 104)), (2, (115, 97)), (2, (114, 116)), (2, (114, 44)), (2, (112, 116)), (2, (111, 116)), (2, (111, 99)), (2, (110, 97)), (2, (109, 112)), (2, (105, 114)), (2, (105, 112)), (2, (105, 102)), (2, (105, 98)), (2, (103, 111)), (2, (103, 97)), (2, (103, 46)), (2, (101, 116)), (2, (101, 113)), (2, (100, 46)), (2, (99, 117)), (2, (77, 115)), (2, (77, 76)), (2, (76, 77)), (2, (76, 76)), (2, (41, 32)), (2, (32, 77)), (2, (32, 76)), (2, (32, 40)), (1, (153, 116)), (1, (153, 115)), (1, (148, 116)), (1, (148, 105)), (1, (122, 105)), (1, (122, 101)), (1, (121, 115)), (1, (121, 111)), (1, (121, 101)), (1, (120, 105)), (1, (119, 111)), (1, (119, 97)), (1, (119, 46)), (1, (116, 119)), (1, (116, 116)), (1, (116, 115)), (1, (116, 46)), (1, (116, 45)), (1, (115, 226)), (1, (115, 117)), (1, (115, 112)), (1, (115, 110)), (1, (115, 107)), (1, (115, 102)), (1, (115, 41)), (1, (114, 121)), (1, (114, 114)), (1, (114, 102)), (1, (114, 100)), (1, (114, 99)), (1, (114, 46)), (1, (112, 117)), (1, (111, 108)), (1, (111, 107)), (1, (110, 226)), (1, (110, 118)), (1, (110, 117)), (1, (110, 108)), (1, (110, 46)), (1, (110, 44)), (1, (109, 115)), (1, (109, 109)), (1, (109, 32)), (1, (108, 114)), (1, (108, 112)), (1, (108, 46)), (1, (107, 115)), (1, (107, 101)), (1, (107, 46)), (1, (107, 44)), (1, (106, 111)), (1, (105, 118)), (1, (105, 109)), (1, (105, 100)), (1, (105, 97)), (1, (104, 117)), (1, (103, 226)), (1, (103, 117)), (1, (103, 115)), (1, (103, 114)), (1, (103, 108)), (1, (102, 97)), (1, (101, 121)), (1, (101, 111)), (1, (101, 104)), (1, (101, 103)), (1, (101, 102)), (1, (101, 58)), (1, (101, 45)), (1, (100, 121)), (1, (100, 111)), (1, (100, 44)), (1, (99, 107)), (1, (99, 99)), (1, (99, 32)), (1, (98, 121)), (1, (98, 114)), (1, (98, 97)), (1, (97, 117)), (1, (97, 106)), (1, (97, 102)), (1, (97, 44)), (1, (87, 104)), (1, (84, 111)), (1, (83, 101)), (1, (77, 111)), (1, (76, 111)), (1, (76, 41)), (1, (76, 32)), (1, (73, 110)), (1, (72, 111)), (1, (70, 105)), (1, (66, 117)), (1, (65, 115)), (1, (65, 110)), (1, (58, 32)), (1, (45, 115)), (1, (45, 103)), (1, (40, 77)), (1, (40, 76)), (1, (32, 121)), (1, (32, 118)), (1, (32, 113)), (1, (32, 107)), (1, (32, 87)), (1, (32, 83)), (1, (32, 72)), (1, (32, 70)), (1, (32, 66))]\nchr(101), chr(32) # most common tokens changed back into characters\n\n('e', ' ')\ntop_ranking_pair = max(most_frequent_pair, key=most_frequent_pair.get) # key=most_frequent_pair.get ranks the keys by the value of the pair\ntop_ranking_pair\n\n(101, 32)\ndef merge_tokens(ids, pair, idx):\n    # in the list of ints (ids), replace all consecutive occurrences of the pair with a new token\n    newids = []\n    i = 0\n    while i &lt; len(ids):\n        # if we are not at the very last position AND the pair matches, replace it\n        if i &lt; len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n            newids.append(idx)\n            i += 2 # skip the next two bytes\n        else:\n            newids.append(ids[i])\n            i += 1\n    return newids\n\nprint(\"length of tokens before merge:\", len(tokens))\nprint(\"---Merging tokens---\")\ntokens_after_merge = merge_tokens(tokens, top_ranking_pair, 255)\nprint(\"---\")\nprint(tokens_after_merge)\nprint(\"length of tokens after merge:\", len(tokens_after_merge))\n\n\nlength of tokens before merge: 2153\n---Merging tokens---\n---\n[84, 104, 255, 115, 99, 97, 108, 105, 110, 103, 32, 117, 112, 32, 111, 102, 32, 65, 73, 32, 109, 111, 100, 101, 108, 115, 32, 104, 97, 115, 32, 116, 119, 111, 32, 109, 97, 106, 111, 114, 32, 99, 111, 110, 115, 101, 113, 117, 101, 110, 99, 101, 115, 46, 32, 70, 105, 114, 115, 116, 44, 32, 65, 73, 32, 109, 111, 100, 101, 108, 115, 32, 97, 114, 255, 98, 101, 99, 111, 109, 105, 110, 103, 32, 109, 111, 114, 255, 112, 111, 119, 101, 114, 102, 117, 108, 32, 97, 110, 100, 32, 99, 97, 112, 97, 98, 108, 255, 111, 102, 32, 109, 111, 114, 255, 116, 97, 115, 107, 115, 44, 32, 101, 110, 97, 98, 108, 105, 110, 103, 32, 109, 111, 114, 255, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 46, 32, 77, 111, 114, 255, 112, 101, 111, 112, 108, 255, 97, 110, 100, 32, 116, 101, 97, 109, 115, 32, 108, 101, 118, 101, 114, 97, 103, 255, 65, 73, 32, 116, 111, 32, 105, 110, 99, 114, 101, 97, 115, 255, 112, 114, 111, 100, 117, 99, 116, 105, 118, 105, 116, 121, 44, 32, 99, 114, 101, 97, 116, 255, 101, 99, 111, 110, 111, 109, 105, 99, 32, 118, 97, 108, 117, 101, 44, 32, 97, 110, 100, 32, 105, 109, 112, 114, 111, 118, 255, 113, 117, 97, 108, 105, 116, 121, 32, 111, 102, 32, 108, 105, 102, 101, 46, 32, 83, 101, 99, 111, 110, 100, 44, 32, 116, 114, 97, 105, 110, 105, 110, 103, 32, 108, 97, 114, 103, 255, 108, 97, 110, 103, 117, 97, 103, 255, 109, 111, 100, 101, 108, 115, 32, 40, 76, 76, 77, 115, 41, 32, 114, 101, 113, 117, 105, 114, 101, 115, 32, 100, 97, 116, 97, 44, 32, 99, 111, 109, 112, 117, 116, 255, 114, 101, 115, 111, 117, 114, 99, 101, 115, 44, 32, 97, 110, 100, 32, 115, 112, 101, 99, 105, 97, 108, 105, 122, 101, 100, 32, 116, 97, 108, 101, 110, 116, 32, 116, 104, 97, 116, 32, 111, 110, 108, 121, 32, 97, 32, 102, 101, 119, 32, 111, 114, 103, 97, 110, 105, 122, 97, 116, 105, 111, 110, 115, 32, 99, 97, 110, 32, 97, 102, 102, 111, 114, 100, 46, 32, 84, 104, 105, 115, 32, 104, 97, 115, 32, 108, 101, 100, 32, 116, 111, 32, 116, 104, 255, 101, 109, 101, 114, 103, 101, 110, 99, 255, 111, 102, 32, 109, 111, 100, 101, 108, 32, 97, 115, 32, 97, 32, 115, 101, 114, 118, 105, 99, 101, 58, 32, 109, 111, 100, 101, 108, 115, 32, 100, 101, 118, 101, 108, 111, 112, 101, 100, 32, 98, 121, 32, 116, 104, 101, 115, 255, 102, 101, 119, 32, 111, 114, 103, 97, 110, 105, 122, 97, 116, 105, 111, 110, 115, 32, 97, 114, 255, 109, 97, 100, 255, 97, 118, 97, 105, 108, 97, 98, 108, 255, 102, 111, 114, 32, 111, 116, 104, 101, 114, 115, 32, 116, 111, 32, 117, 115, 255, 97, 115, 32, 97, 32, 115, 101, 114, 118, 105, 99, 101, 46, 32, 65, 110, 121, 111, 110, 255, 119, 104, 111, 32, 119, 105, 115, 104, 101, 115, 32, 116, 111, 32, 108, 101, 118, 101, 114, 97, 103, 255, 65, 73, 32, 116, 111, 32, 98, 117, 105, 108, 100, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 32, 99, 97, 110, 32, 110, 111, 119, 32, 117, 115, 255, 116, 104, 101, 115, 255, 109, 111, 100, 101, 108, 115, 32, 116, 111, 32, 100, 111, 32, 115, 111, 32, 119, 105, 116, 104, 111, 117, 116, 32, 104, 97, 118, 105, 110, 103, 32, 116, 111, 32, 105, 110, 118, 101, 115, 116, 32, 117, 112, 32, 102, 114, 111, 110, 116, 32, 105, 110, 32, 98, 117, 105, 108, 100, 105, 110, 103, 32, 97, 32, 109, 111, 100, 101, 108, 46, 32, 73, 110, 32, 115, 104, 111, 114, 116, 44, 32, 116, 104, 255, 100, 101, 109, 97, 110, 100, 32, 102, 111, 114, 32, 65, 73, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 32, 104, 97, 115, 32, 105, 110, 99, 114, 101, 97, 115, 101, 100, 32, 119, 104, 105, 108, 255, 116, 104, 255, 98, 97, 114, 114, 105, 101, 114, 32, 116, 111, 32, 101, 110, 116, 114, 121, 32, 102, 111, 114, 32, 98, 117, 105, 108, 100, 105, 110, 103, 32, 65, 73, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 32, 104, 97, 115, 32, 100, 101, 99, 114, 101, 97, 115, 101, 100, 46, 32, 84, 104, 105, 115, 32, 104, 97, 115, 32, 116, 117, 114, 110, 101, 100, 32, 65, 73, 32, 101, 110, 103, 105, 110, 101, 101, 114, 105, 110, 103, 226, 128, 148, 116, 104, 255, 112, 114, 111, 99, 101, 115, 115, 32, 111, 102, 32, 98, 117, 105, 108, 100, 105, 110, 103, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 32, 111, 110, 32, 116, 111, 112, 32, 111, 102, 32, 114, 101, 97, 100, 105, 108, 121, 32, 97, 118, 97, 105, 108, 97, 98, 108, 255, 109, 111, 100, 101, 108, 115, 226, 128, 148, 105, 110, 116, 111, 32, 111, 110, 255, 111, 102, 32, 116, 104, 255, 102, 97, 115, 116, 101, 115, 116, 45, 103, 114, 111, 119, 105, 110, 103, 32, 101, 110, 103, 105, 110, 101, 101, 114, 105, 110, 103, 32, 100, 105, 115, 99, 105, 112, 108, 105, 110, 101, 115, 46, 32, 66, 117, 105, 108, 100, 105, 110, 103, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 32, 111, 110, 32, 116, 111, 112, 32, 111, 102, 32, 109, 97, 99, 104, 105, 110, 255, 108, 101, 97, 114, 110, 105, 110, 103, 32, 40, 77, 76, 41, 32, 109, 111, 100, 101, 108, 115, 32, 105, 115, 110, 226, 128, 153, 116, 32, 110, 101, 119, 46, 32, 76, 111, 110, 103, 32, 98, 101, 102, 111, 114, 255, 76, 76, 77, 115, 32, 98, 101, 99, 97, 109, 255, 112, 114, 111, 109, 105, 110, 101, 110, 116, 44, 32, 65, 73, 32, 119, 97, 115, 32, 97, 108, 114, 101, 97, 100, 121, 32, 112, 111, 119, 101, 114, 105, 110, 103, 32, 109, 97, 110, 121, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 44, 32, 105, 110, 99, 108, 117, 100, 105, 110, 103, 32, 112, 114, 111, 100, 117, 99, 116, 32, 114, 101, 99, 111, 109, 109, 101, 110, 100, 97, 116, 105, 111, 110, 115, 44, 32, 102, 114, 97, 117, 100, 32, 100, 101, 116, 101, 99, 116, 105, 111, 110, 44, 32, 97, 110, 100, 32, 99, 104, 117, 114, 110, 32, 112, 114, 101, 100, 105, 99, 116, 105, 111, 110, 46, 32, 87, 104, 105, 108, 255, 109, 97, 110, 121, 32, 112, 114, 105, 110, 99, 105, 112, 108, 101, 115, 32, 111, 102, 32, 112, 114, 111, 100, 117, 99, 116, 105, 111, 110, 105, 122, 105, 110, 103, 32, 65, 73, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 115, 32, 114, 101, 109, 97, 105, 110, 32, 116, 104, 255, 115, 97, 109, 101, 44, 32, 116, 104, 255, 110, 101, 119, 32, 103, 101, 110, 101, 114, 97, 116, 105, 111, 110, 32, 111, 102, 32, 108, 97, 114, 103, 101, 45, 115, 99, 97, 108, 101, 44, 32, 114, 101, 97, 100, 105, 108, 121, 32, 97, 118, 97, 105, 108, 97, 98, 108, 255, 109, 111, 100, 101, 108, 115, 32, 98, 114, 105, 110, 103, 115, 32, 97, 98, 111, 117, 116, 32, 110, 101, 119, 32, 112, 111, 115, 115, 105, 98, 105, 108, 105, 116, 105, 101, 115, 32, 97, 110, 100, 32, 110, 101, 119, 32, 99, 104, 97, 108, 108, 101, 110, 103, 101, 115, 44, 32, 119, 104, 105, 99, 104, 32, 97, 114, 255, 116, 104, 255, 102, 111, 99, 117, 115, 32, 111, 102, 32, 116, 104, 105, 115, 32, 98, 111, 111, 107, 46, 32, 84, 104, 105, 115, 32, 99, 104, 97, 112, 116, 101, 114, 32, 98, 101, 103, 105, 110, 115, 32, 119, 105, 116, 104, 32, 97, 110, 32, 111, 118, 101, 114, 118, 105, 101, 119, 32, 111, 102, 32, 102, 111, 117, 110, 100, 97, 116, 105, 111, 110, 32, 109, 111, 100, 101, 108, 115, 44, 32, 116, 104, 255, 107, 101, 121, 32, 99, 97, 116, 97, 108, 121, 115, 116, 32, 98, 101, 104, 105, 110, 100, 32, 116, 104, 255, 101, 120, 112, 108, 111, 115, 105, 111, 110, 32, 111, 102, 32, 65, 73, 32, 101, 110, 103, 105, 110, 101, 101, 114, 105, 110, 103, 46, 32, 73, 226, 128, 153, 108, 108, 32, 116, 104, 101, 110, 32, 100, 105, 115, 99, 117, 115, 115, 32, 97, 32, 114, 97, 110, 103, 255, 111, 102, 32, 115, 117, 99, 99, 101, 115, 115, 102, 117, 108, 32, 65, 73, 32, 117, 115, 255, 99, 97, 115, 101, 115, 44, 32, 101, 97, 99, 104, 32, 105, 108, 108, 117, 115, 116, 114, 97, 116, 105, 110, 103, 32, 119, 104, 97, 116, 32, 65, 73, 32, 105, 115, 32, 103, 111, 111, 100, 32, 97, 110, 100, 32, 110, 111, 116, 32, 121, 101, 116, 32, 103, 111, 111, 100, 32, 97, 116, 46, 32, 65, 115, 32, 65, 73, 226, 128, 153, 115, 32, 99, 97, 112, 97, 98, 105, 108, 105, 116, 105, 101, 115, 32, 101, 120, 112, 97, 110, 100, 32, 100, 97, 105, 108, 121, 44, 32, 112, 114, 101, 100, 105, 99, 116, 105, 110, 103, 32, 105, 116, 115, 32, 102, 117, 116, 117, 114, 255, 112, 111, 115, 115, 105, 98, 105, 108, 105, 116, 105, 101, 115, 32, 98, 101, 99, 111, 109, 101, 115, 32, 105, 110, 99, 114, 101, 97, 115, 105, 110, 103, 108, 121, 32, 99, 104, 97, 108, 108, 101, 110, 103, 105, 110, 103, 46, 32, 72, 111, 119, 101, 118, 101, 114, 44, 32, 101, 120, 105, 115, 116, 105, 110, 103, 32, 97, 112, 112, 108, 105, 99, 97, 116, 105, 111, 110, 32, 112, 97, 116, 116, 101, 114, 110, 115, 32, 99, 97, 110, 32, 104, 101, 108, 112, 32, 117, 110, 99, 111, 118, 101, 114, 32, 111, 112, 112, 111, 114, 116, 117, 110, 105, 116, 105, 101, 115, 32, 116, 111, 100, 97, 121, 32, 97, 110, 100, 32, 111, 102, 102, 101, 114, 32, 99, 108, 117, 101, 115, 32, 97, 98, 111, 117, 116, 32, 104, 111, 119, 32, 65, 73, 32, 109, 97, 121, 32, 99, 111, 110, 116, 105, 110, 117, 255, 116, 111, 32, 98, 255, 117, 115, 101, 100, 32, 105, 110, 32, 116, 104, 255, 102, 117, 116, 117, 114, 101, 46, 32, 84, 111, 32, 99, 108, 111, 115, 255, 111, 117, 116, 32, 116, 104, 255, 99, 104, 97, 112, 116, 101, 114, 44, 32, 73, 226, 128, 153, 108, 108, 32, 112, 114, 111, 118, 105, 100, 255, 97, 110, 32, 111, 118, 101, 114, 118, 105, 101, 119, 32, 111, 102, 32, 116, 104, 255, 110, 101, 119, 32, 65, 73, 32, 115, 116, 97, 99, 107, 44, 32, 105, 110, 99, 108, 117, 100, 105, 110, 103, 32, 119, 104, 97, 116, 32, 104, 97, 115, 32, 99, 104, 97, 110, 103, 101, 100, 32, 119, 105, 116, 104, 32, 102, 111, 117, 110, 100, 97, 116, 105, 111, 110, 32, 109, 111, 100, 101, 108, 115, 44, 32, 119, 104, 97, 116, 32, 114, 101, 109, 97, 105, 110, 115, 32, 116, 104, 255, 115, 97, 109, 101, 44, 32, 97, 110, 100, 32, 104, 111, 119, 32, 116, 104, 255, 114, 111, 108, 255, 111, 102, 32, 97, 110, 32, 65, 73, 32, 101, 110, 103, 105, 110, 101, 101, 114, 32, 116, 111, 100, 97, 121, 32, 100, 105, 102, 102, 101, 114, 115, 32, 102, 114, 111, 109, 32, 116, 104, 97, 116, 32, 111, 102, 32, 97, 32, 116, 114, 97, 100, 105, 116, 105, 111, 110, 97, 108, 32, 77, 76, 32, 101, 110, 103, 105, 110, 101, 101, 114, 46]\nlength of tokens after merge: 2096\nKeep iterating over the next largest pair to mint\nvocab_size = 276 # desired final volcabulary size \nnumber_of_merges = vocab_size - 256 # number of merges to perform\nids = list(tokens) # copy so we don't destory original list\n\n# building a BINARY TREE of merges (starting not from root but from the leaves)\nmerges = {} # (child1, child2) -&gt; mapping to new token\nfor i in range(number_of_merges):\n  most_frequent_pair = get_most_frequent_pair(ids) # find most commonly occuring pair\n  pair = max(most_frequent_pair, key=most_frequent_pair.get)\n  idx = 256 + i\n  print(f\"merging {pair} into new token {idx}\")\n  ids = merge_tokens(ids, pair, idx)\n  merges[pair] = idx\n\nmerging (101, 32) into new token 256\nmerging (115, 32) into new token 257\nmerging (105, 110) into new token 258\nmerging (111, 110) into new token 259\nmerging (32, 116) into new token 260\nmerging (32, 97) into new token 261\nmerging (97, 116) into new token 262\nmerging (258, 103) into new token 263\nmerging (101, 114) into new token 264\nmerging (105, 259) into new token 265\nmerging (111, 100) into new token 266\nmerging (111, 102) into new token 267\nmerging (114, 101) into new token 268\nmerging (105, 108) into new token 269\nmerging (104, 256) into new token 270\nmerging (65, 73) into new token 271\nmerging (101, 110) into new token 272\nmerging (100, 32) into new token 273\nmerging (32, 109) into new token 274\nmerging (46, 32) into new token 275\nNote that some prior minted tokens are also eligible for merging\nprint(len(tokens))\nprint(len(ids))\nprint(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")\n\n2153\n1653\ncompression ratio: 1.30X\nThe Technical User’s Introduction to LLM Tokenization - Christopher Samiullah\nThe Tokenizer is completely seperate, independent module to the LLM. It has its own training dataset of text, on which on train the vocabulary using the Byte-Pair Encoding Algorithm.\nThe Tokenizer then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never deals with any text",
    "crumbs": [
      "Tokenization"
    ]
  },
  {
    "objectID": "topics/tokenization.html#encoding",
    "href": "topics/tokenization.html#encoding",
    "title": "Tokenization",
    "section": "Encoding",
    "text": "Encoding\nGiven a string, what are the tokens?\n\ndef encode(text):\n    tokens = list(text.encode('utf-8'))\n    while True:\n        # this time we dont care for frequency of pairs\n        raw_pairs_in_sequence = get_most_frequent_pair(tokens)\n        # get the lowest index pair (earliest merges first\n        # all pairs inside raw_pairs_in_sequence, get index, get pair with min number index\n        # float inf is fallback for non merging pairs\n        # pair returns the most eligible merging pair from merged list\n        pair = min(raw_pairs_in_sequence, key=lambda p: merges.get(p, float('inf')))\n        if pair not in merges:\n            break  # nothing else can be merged\n        idx = merges[pair]\n        tokens = merge_tokens(tokens, pair, idx)\n    return tokens\nprint(encode(\"hello, world!\"))\n\n[104, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33]",
    "crumbs": [
      "Tokenization"
    ]
  },
  {
    "objectID": "topics/linear-regression.html",
    "href": "topics/linear-regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Minimising the loss function\nA Practical Guide to Machine Learning with TensorFlow 2.0 & Keras by Vadim Karpusenko\nA Tensor is an Array/ Matrix.\nTransform a Tensor representation into a numpy representation\nTensorflow is trying to optimise the utilisation of the hardware. Tensorflow keeps track of operations and how they’re running on the GPU. Helping the most compute intesive parts - using pre built libraries for these operations instead.\nLet’s create noisy data (100 points) in form of m * X + b = Y:\nThe distribution is more or less equal (uniform)\nThe problem we are trying to solve is funding the correlation, fitting the red line to our green line. Getting the red line to fit the optimal solution (the green line point)\nWe need to find the distance between our red line and the green line. We need to find our error - how far are we from correct answer.\nFinding the loss (distance between the line and a point).\nFind distance between prediction of Y and true value of Y\nThe loss is significant 0.1258082\nWe could do this by hand, changing the weights and biases, twiddling them ourselves…however that would take forever! So we need an automatic solution with differentiation\nWe define the learning_rate. We want our steps to be quite small to prevent overshooting the Gradient Descent (we want to avoid jumping out of our local minima).\nThis will be an iterative method to get us close as we can. - Calculate current loss function - Decide what direction to move - Move in that direction\nGradientTape() is a Tensorflow operation where TF keeps track of mathematical operations\nlearning_rate = 0.05\nsteps = 200\n\nfor i in range(steps):\n\n    with tf.GradientTape() as tape:\n        predictions = predict(X)\n        loss = mean_squared_error(predictions, Y)\n\n    gradients = tape.gradient(loss, [w, b])\n\n    w.assign_sub(gradients[0] * learning_rate)\n    b.assign_sub(gradients[1] * learning_rate)\n\n    if i % 20 == 0:\n        print(\"Step %d, Loss %f\" % (i, loss.numpy()))\n\nStep 0, Loss 0.124257\nStep 20, Loss 0.000795\nStep 40, Loss 0.000184\nStep 60, Loss 0.000162\nStep 80, Loss 0.000148\nStep 100, Loss 0.000137\nStep 120, Loss 0.000128\nStep 140, Loss 0.000121\nStep 160, Loss 0.000116\nStep 180, Loss 0.000112\nMatching our original values makes our loss far far lower 0.000112. We are closer to our ideal solution.\ngradients\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0014718910679221153&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.000763709656894207&gt;]\nw\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.11502755433320999&gt;\nb\n\n&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.29490748047828674&gt;\nw_true = 0.1\nb_true = 0.3\nplt.plot(X, Y, \"b.\")\nplt.plot([0, 1], [0*w+b, 1*w+b], \"r:\") \nplt.plot([0, 1], [0*w_true+b_true, 1*w_true+b_true], \"g:\")\nplt.show()\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nws = np.linspace(-1, 1)\nbs = np.linspace(-1, 1)\nw_mesh, b_mesh = np.meshgrid(ws, bs)\n\n\ndef loss_for_values(w, b):\n    y = w * X + b\n    loss = mean_squared_error(y, Y)\n    return loss\n\n\nzs = np.array([\n    loss_for_values(w, b) for (w, b) in zip(np.ravel(w_mesh), np.ravel(b_mesh))\n])\nz_mesh = zs.reshape(w_mesh.shape)\n\nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(w_mesh, b_mesh, z_mesh, color='b', alpha=0.06)\n\nw = tf.Variable(-.5)\nb = tf.Variable(-.75)\n\nhistory = []\n\nfor i in range(steps):\n    with tf.GradientTape() as tape:\n        predictions = predict(X)\n        loss = mean_squared_error(predictions, Y)\n    gradients = tape.gradient(loss, [w, b])\n    history.append((w.numpy(), b.numpy(), loss.numpy()))\n    w.assign_sub(gradients[0] * learning_rate)\n    b.assign_sub(gradients[1] * learning_rate)\n\n# Plot the trajectory\nax.plot([h[0] for h in history], [h[1] for h in history],\n        [h[2] for h in history],\n        marker='o')\n\nax.set_xlabel('w', fontsize=18, labelpad=20)\nax.set_ylabel('b', fontsize=18, labelpad=20)\nax.set_zlabel('loss', fontsize=18, labelpad=20)\n\nax.view_init(elev=22, azim=25)\nplt.show()",
    "crumbs": [
      "Linear Regression"
    ]
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "2  Books",
    "section": "",
    "text": "Not started  In progress  Completed\n\n\n\n\n\n\n\n\nNo items found.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Books</span>"
    ]
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "3  Papers",
    "section": "",
    "text": "Not started  In progress  Completed\n\n\n\n\n\n\n\n\nNo items found.\n\n\n\n\nBlog posts\n\n\n\n\n\n\n\n\nNo items found.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Papers</span>"
    ]
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "4  Courses",
    "section": "",
    "text": "Not started  In progress  Completed\n\n\n\n\n\n\n\n\nNo items found.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Courses</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Daily / Weekly\nQuick access to frequently used links. Add/edit freely.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#daily-weekly",
    "href": "resources.html#daily-weekly",
    "title": "Resources",
    "section": "",
    "text": "Latent Space\nSam Altman’s Blog\nNews from Wes Roth",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#llms-genai",
    "href": "resources.html#llms-genai",
    "title": "Resources",
    "section": "LLMs / GenAI",
    "text": "LLMs / GenAI\n\nAndrej Karpathy’s Youtube\n3 Blue 1 Brown Youtube\nHugging Face Youtube",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#python-data",
    "href": "resources.html#python-data",
    "title": "Resources",
    "section": "Python / Data",
    "text": "Python / Data\n\nTensorflow examples",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#model-training",
    "href": "resources.html#model-training",
    "title": "Resources",
    "section": "Model Training",
    "text": "Model Training\n\nNatural Language Toolkit\nSpacy\nTools-to-Design-or-Visualize-Architecture-of-Neural-Network",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#research-learning",
    "href": "resources.html#research-learning",
    "title": "Resources",
    "section": "Research & Learning",
    "text": "Research & Learning\n\narc prize",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#tools-platforms",
    "href": "resources.html#tools-platforms",
    "title": "Resources",
    "section": "Tools & Platforms",
    "text": "Tools & Platforms\n\nTensorFlow Playground\nRun local AI models like gpt-oss, Qwen, Gemma, DeepSeek and many more on your computer, privately and for free\nlmarena\nInference Playground",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#newsletters",
    "href": "resources.html#newsletters",
    "title": "Resources",
    "section": "Newsletters",
    "text": "Newsletters\n\nLatent Space\nnews.smol.ai",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "topics/convolutional-neural-networks.html",
    "href": "topics/convolutional-neural-networks.html",
    "title": "2  Convolutional Neural Networks",
    "section": "",
    "text": "The standard when it comes to Computer vision\nConvolutional Layer can detect abstract ideas in an image\n\nImage Classification\nObject Detection\n\nOutput of Convolution is called a Feature Map (a mapping from the activations of detected features from the input layer )\nA Practical Guide to Machine Learning with TensorFlow 2.0 & Keras\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n/Users/charlotte/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\n\n\nprint(tf.__version__)\nprint(keras.__version__)\n\n2.20.0\n3.10.0\n\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\nx_train input data, y_train labels\n\nimport numpy as np\nnp.set_printoptions(linewidth=5000)\nx_train[0]\n\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=uint8)\n\n\n0 is black, numbers closer to 255 are increasing in intensity to white\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.imshow(x_train[0], cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(y_train[0])\n\n5\n\n\nWe need to work with floating point numbers rather than 0 - 255\n\nx_train = x_train / 255.0\nx_test = x_test / 255.0\nx_train[0]\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.01176471, 0.07058824, 0.07058824, 0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078, 0.65098039, 1.        , 0.96862745, 0.49803922, 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.19215686, 0.93333333, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549, 0.96862745, 0.94509804, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.05490196, 0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098, 0.42352941, 0.00392157, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.17647059, 0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.97647059, 0.99215686, 0.97647059, 0.25098039, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.18039216, 0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.15294118, 0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706, 0.00784314, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549, 0.03529412, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.21568627, 0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ]])\n\n\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten \n\nDense - all neurons in one layer will be connected to neurons of the next layer\nFlatten - 2D image flattened\n\nOne long input array of 784 pixels\ninput_shape is telling the model what data structure to expect\nNo additional parameters (weights or biases) in this layer\n\n\nmodel = Sequential()\n\nmodel.add(Flatten(input_shape=(28, 28)))\nmodel.summary()\n\n/Users/charlotte/Library/Python/3.9/lib/python/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (Flatten)               │ (None, 784)            │             0 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 0 (0.00 B)\n\n\n\n Trainable params: 0 (0.00 B)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nmodel.add(Dense(units=256, activation=\"relu\"))\nmodel.add(Dense(units=10, activation=\"softmax\"))\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ flatten (Flatten)               │ (None, 784)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 256)            │       200,960 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 10)             │         2,570 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 203,530 (795.04 KB)\n\n\n\n Trainable params: 203,530 (795.04 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nevery neuron will see the whole image (all 256 pixels)\nthe following layer will only have 10 neurons however these are connected to all 256 neurons\n\n\nimport visualkeras\nvisualkeras.layered_view(model)\n\n\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[17], line 2\n      1 import visualkeras\n----&gt; 2 visualkeras.layered_view(model)\n\nFile ~/Library/Python/3.9/lib/python/site-packages/visualkeras/layered.py:290, in layered_view(model, to_file, min_z, min_xy, max_z, max_xy, scale_z, scale_xy, type_ignore, index_ignore, color_map, one_dim_orientation, index_2D, background_fill, draw_volume, draw_reversed, padding, text_callable, text_vspacing, spacing, draw_funnel, shade_step, legend, legend_text_spacing_offset, font, font_color, show_dimension, sizing_mode, dimension_caps, relative_base_size, options, preset)\n    287         layer_name = f'unknown_layer_{index}'\n    289 # Get the primary shape of the layer's output\n--&gt; 290 shape = extract_primary_shape(layer.output_shape, layer_name)\n    292 # Calculate dimensions with flexible sizing\n    293 x, y, z = calculate_layer_dimensions(\n    294     shape, scale_z, scale_xy,\n    295     max_z, max_xy, min_z, min_xy,\n    296     one_dim_orientation, sizing_mode,\n    297     dimension_caps, relative_base_size\n    298 )\n\nAttributeError: 'Flatten' object has no attribute 'output_shape'\n\n\n\n\n\n\nfrom tensorflow import keras\nimport visualkeras\nfrom IPython.display import display\n\n# Reuse existing `model` if defined; otherwise create the MNIST MLP\ntry:\n    model  # noqa: F821\nexcept NameError:\n    model = keras.Sequential([\n        keras.Input(shape=(28, 28)),\n        keras.layers.Flatten(),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dense(10, activation=\"softmax\"),\n    ])\n\nimg = visualkeras.layered_view(\n    model,\n    legend=True,\n    show_dimension=True,\n    sizing_mode=\"balanced\",\n)\n\ndisplay(img)\n\n# Save a copy to your site images folder\nimg.save(\"/Users/charlotte/charlotte.computer/images/mnist_mlp.png\")\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[18], line 16\n      8 except NameError:\n      9     model = keras.Sequential([\n     10         keras.Input(shape=(28, 28)),\n     11         keras.layers.Flatten(),\n     12         keras.layers.Dense(256, activation=\"relu\"),\n     13         keras.layers.Dense(10, activation=\"softmax\"),\n     14     ])\n---&gt; 16 img = visualkeras.layered_view(\n     17     model,\n     18     legend=True,\n     19     show_dimension=True,\n     20     sizing_mode=\"balanced\",\n     21 )\n     23 display(img)\n     25 # Save a copy to your site images folder\n\nFile ~/Library/Python/3.9/lib/python/site-packages/visualkeras/layered.py:290, in layered_view(model, to_file, min_z, min_xy, max_z, max_xy, scale_z, scale_xy, type_ignore, index_ignore, color_map, one_dim_orientation, index_2D, background_fill, draw_volume, draw_reversed, padding, text_callable, text_vspacing, spacing, draw_funnel, shade_step, legend, legend_text_spacing_offset, font, font_color, show_dimension, sizing_mode, dimension_caps, relative_base_size, options, preset)\n    287         layer_name = f'unknown_layer_{index}'\n    289 # Get the primary shape of the layer's output\n--&gt; 290 shape = extract_primary_shape(layer.output_shape, layer_name)\n    292 # Calculate dimensions with flexible sizing\n    293 x, y, z = calculate_layer_dimensions(\n    294     shape, scale_z, scale_xy,\n    295     max_z, max_xy, min_z, min_xy,\n    296     one_dim_orientation, sizing_mode,\n    297     dimension_caps, relative_base_size\n    298 )\n\nAttributeError: 'Flatten' object has no attribute 'output_shape'\n\n\n\n\n%pip install -U pydot graphviz\n\n\n\nimport keras\nfrom keras.utils import plot_model\nfrom IPython.display import Image, display\n\n# Reuse existing `model` if defined; otherwise create the MNIST MLP\ntry:\n    model  # noqa: F821\nexcept NameError:\n    model = keras.Sequential([\n        keras.Input(shape=(28, 28)),\n        keras.layers.Flatten(),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dense(10, activation=\"softmax\"),\n    ])\n\nout_path = \"/Users/charlotte/charlotte.computer/images/mnist_mlp_graph.png\"\nplot_model(\n    model,\n    to_file=out_path,\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"LR\",\n    dpi=200,\n)\n\ndisplay(Image(filename=out_path))\n\n\nYou must install pydot (`pip install pydot`) for `plot_model` to work.\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[19], line 26\n     16 out_path = \"/Users/charlotte/charlotte.computer/images/mnist_mlp_graph.png\"\n     17 plot_model(\n     18     model,\n     19     to_file=out_path,\n   (...)\n     23     dpi=200,\n     24 )\n---&gt; 26 display(Image(filename=out_path))\n\nFile ~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:970, in Image.__init__(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\n    968 self.unconfined = unconfined\n    969 self.alt = alt\n--&gt; 970 super(Image, self).__init__(data=data, url=url, filename=filename,\n    971         metadata=metadata)\n    973 if self.width is None and self.metadata.get('width', {}):\n    974     self.width = metadata['width']\n\nFile ~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:327, in DisplayObject.__init__(self, data, url, filename, metadata)\n    324 elif self.metadata is None:\n    325     self.metadata = {}\n--&gt; 327 self.reload()\n    328 self._check_data()\n\nFile ~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:1005, in Image.reload(self)\n   1003 \"\"\"Reload the raw data from file or URL.\"\"\"\n   1004 if self.embed:\n-&gt; 1005     super(Image,self).reload()\n   1006     if self.retina:\n   1007         self._retina_shape()\n\nFile ~/Library/Python/3.9/lib/python/site-packages/IPython/core/display.py:353, in DisplayObject.reload(self)\n    351 if self.filename is not None:\n    352     encoding = None if \"b\" in self._read_flags else \"utf-8\"\n--&gt; 353     with open(self.filename, self._read_flags, encoding=encoding) as f:\n    354         self.data = f.read()\n    355 elif self.url is not None:\n    356     # Deferred import\n\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/charlotte/charlotte.computer/images/mnist_mlp_graph.png'",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Convolutional Neural Networks</span>"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Keras Datasets",
    "crumbs": [
      "Datasets"
    ]
  }
]